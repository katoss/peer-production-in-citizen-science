---
layout: page
title: Public Lab Case Study
---

## Introduction
<a href="https://publiclab.org">Public Lab</a>, short for Public Laboratory for Open Technology and Science, is a community, non-profit organization and online platform, aiming to empower people to address environmental justice issues through community science and open technology. They focus on the collaborative development, sharing, reuse and adaptation of DIY scientific tools intended to enable community investigation of environmental concerns. Public Lab was formed after the BP oil disaster at the Gulf of Mexico in 2010, when residents documented damage in a grassroot mapping effort using weather balloons (Breen et al., 2019). As of September 2022, there were over 2,100 registered users, who contributed numerous topic pages, reports, and manuals for do-it-yourself environmental monitoring projects. In April 2023, Public Lab announced that due to economic reasons, they would be discontinuing staff support and closing their online store. They are now striving to sustain operations through a volunteer-run model.

## Case study
### Common research object
In Public Lab, the research object is a catalog of experimental designs and project reports in the domain of Do-It-Yourself environmental science. Unlike on Zooniverse and iNaturalist, there is no data collection or annotation mechanism on the platform. Instead, artifacts include collections about DIY environmental monitoring and knowledge transfer, like topic pages, similar to wiki pages, e.g. on air quality, and research notes, similar to blog posts (e.g. “How to Build a Bucket Air Monitor”). There are artifacts for problem solving, like issue briefs, in which users explain their issue to support from other users, or more general question elements that other users can reply to. Artifacts are publicly shared and openly licensed by default, although exceptions may occur if appropriate in context, e.g. if shared information risks the safety of individuals (Rey-Mazón et al., 2018).

### Range of tasks
Reflecting the diversity of research artifacts, Public Lab users can work on a range of tasks that can be split roughly in two categories: looking for information on experimental design, and sharing information or outcomes (see Figure 2.2. B). To look for information, users can create dedicated artifacts, i.e. questions and issue briefs, or consult other existing artifacts on their topic of interest. To share outcomes or other information, users can create or contribute to topic or project pages, research notes, DIY toolkits, and blog posts. They can replicate other users’ experiments (“activities”), comment and reply to questions, issue briefs, or research notes.

### Granularity and modularity
The range of tasks comes with varying granularity, ranging from small edits on topic wiki pages to uploading larger building blocks such as experimental designs. Integration of modules is achieved by interlinking contributions on Public Lab through tags and backlinks. These tags need to be added to an artifact to link it to a topic or artifact category, to add backlink functionality (“i did this” / “seeks:replications”, “activity:, “replication:”, “activity:infragram”) to integrate their artifacts / make them findable. Tags are pre-specified if an artifact is created by clicking on “add an activity”, “I did this”, “ask a question” from a related page (see Figure A2.2. in Appendix A.2.). 

### Equipotential self-selection
Similar to iNaturalist, Public Lab users have access to all content to self-select open tasks they are most interested in or add new content to their liking. Upon application, users can take on moderator or administrator roles. 

### Quality control
Due to the free, textual form of the research artifacts, there is no algorithmic quality control that measures consensus like in iNaturalist or Zooniverse. Rather, feedback is generally in written form of comments to other artifacts, or in the form of new blog entries, when users replicate each other’s activities and report their protocol and how it worked in their case.

### Learning trajectories
For new users, there are so-called issue brief artifacts to remove barriers and encourage users to get started (see Figure A2.3.a in Appendix A.2.), “first-time-posters” tag, templates and forms for artifacts, and helpful instructions in default texts of text boxes (see Figure A2.3.b in Appendix A.2.). There are also question objects to help resolve specific issues. It is possible to replicate existing experiments or ask other users to replicate one’s own experiment to get feedback and learn from one’s own or other people’s experiences. More experienced users can apply for moderation or administration roles.

### Direct and indirect coordination
There are several features that help signal open work, like the “I did this” button (see Figure A2.2. a in Appendix A.2.), that appears on pages tagged with “activity”, which depicts an invitation for other users to replicate the experiment, and at the same time serves to connect the original and replicated activity. Other ways to signal that help is needed are dedicated artifacts like “questions” or “issue briefs” (see Figure A2.2.b and A2.3. a) in Appendix A.2. For direct communication, there is a comment functionality under all types of artifacts (see Figure A2.3.b in Appendix A.2.).